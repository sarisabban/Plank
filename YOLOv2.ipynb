{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "cPkBPFUSrLWh",
        "5MF__wQdrU0I",
        "RSQTXkrArXDb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cPkBPFUSrLWh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Start"
      ]
    },
    {
      "metadata": {
        "id": "96mYmxdcqplE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Install dependencies and download pre-trained weights"
      ]
    },
    {
      "metadata": {
        "id": "aq6ySdl1EN06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install imgaug\n",
        "!wget https://pjreddie.com/media/files/yolov2.weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EuA3DNqUCL6X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Upload dataset"
      ]
    },
    {
      "metadata": {
        "id": "Gqfmtm2k-KUS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPAMKoDmCO_g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uncompress dataset"
      ]
    },
    {
      "metadata": {
        "id": "J2qnmdng-TXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -jxvf dataset.tar.bz2\n",
        "!rm dataset.tar.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O4sNWfoRrYVZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup neural network"
      ]
    },
    {
      "metadata": {
        "id": "L0JZgbGpD0AO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/python3\n",
        "\n",
        "'''\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2017 Ngoc Anh Huynh\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "This script is modified from https://github.com/experiencor/keras-yolo2\n",
        "'''\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import copy\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from imgaug import augmenters as iaa\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D\n",
        "\n",
        "image_path\t\t\t= './dataset/images/'\n",
        "annot_path\t\t\t= './dataset/annotations/'\n",
        "wt_path\t\t\t\t= './yolov2.weights'\n",
        "LABELS\t\t\t\t= ['Cell']\n",
        "IMAGE_H, IMAGE_W\t= 416, 416\n",
        "GRID_H, GRID_W\t\t= 13, 13\n",
        "BOX\t\t\t\t\t= 5\n",
        "CLASS\t\t\t\t= len(LABELS)\n",
        "CLASS_WEIGHTS\t\t= np.ones(CLASS, dtype='float32')\n",
        "ANCHORS\t\t\t\t= [\t0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
        "OBJ_THRESHOLD\t\t= 0.3\n",
        "NMS_THRESHOLD\t\t= 0.3\n",
        "NO_OBJECT_SCALE\t\t= 1.0\n",
        "OBJECT_SCALE\t\t= 5.0\n",
        "COORD_SCALE\t\t\t= 1.0\n",
        "CLASS_SCALE\t\t\t= 1.0\n",
        "BATCH_SIZE\t\t\t= 16\n",
        "WARM_UP_BATCHES\t\t= 3\n",
        "TRUE_BOX_BUFFER\t\t= 50\n",
        "GenConf \t\t\t= {\t'IMAGE_H'\t\t\t: IMAGE_H,\n",
        "\t\t\t\t\t\t'IMAGE_W'\t\t\t: IMAGE_W,\n",
        "\t\t\t\t\t\t'GRID_H'\t\t\t: GRID_H,\n",
        "\t\t\t\t\t\t'GRID_W'\t\t\t: GRID_W,\n",
        "\t\t\t\t\t\t'BOX'\t\t\t\t: BOX,\n",
        "\t\t\t\t\t\t'LABELS'\t\t\t: LABELS,\n",
        "\t\t\t\t\t\t'CLASS'\t\t\t\t: len(LABELS),\n",
        "\t\t\t\t\t\t'ANCHORS'\t\t\t: ANCHORS,\n",
        "\t\t\t\t\t\t'BATCH_SIZE'\t\t: BATCH_SIZE,\n",
        "\t\t\t\t\t\t'TRUE_BOX_BUFFER'\t: 50,}\n",
        "\n",
        "os.makedirs('./logs', exist_ok=True)\n",
        "\n",
        "class BoundBox:\n",
        "\tdef __init__(self, xmin, ymin, xmax, ymax, c=None, classes=None):\n",
        "\t\tself.xmin = xmin\n",
        "\t\tself.ymin = ymin\n",
        "\t\tself.xmax = xmax\n",
        "\t\tself.ymax = ymax\n",
        "\t\tself.c = c\n",
        "\t\tself.classes = classes\n",
        "\t\tself.label = -1\n",
        "\t\tself.score = -1\n",
        "\tdef get_label(self):\n",
        "\t\tif self.label == -1:\n",
        "\t\t\tself.label = np.argmax(self.classes)\n",
        "\t\treturn(self.label)\n",
        "\tdef get_score(self):\n",
        "\t\tif self.score == -1:\n",
        "\t\t\tself.score = self.classes[self.get_label()]\n",
        "\t\treturn(self.score)\n",
        "\n",
        "class WeightReader:\n",
        "\tdef __init__(self, weight_file):\n",
        "\t\tself.offset = 4\n",
        "\t\tself.all_weights = np.fromfile(weight_file, dtype='float32')\n",
        "\tdef read_bytes(self, size):\n",
        "\t\tself.offset = self.offset + size\n",
        "\t\treturn(self.all_weights[self.offset-size:self.offset])\n",
        "\tdef reset(self):\n",
        "\t\tself.offset = 4\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "\tintersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "\tintersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\tintersect = intersect_w * intersect_h\n",
        "\tw1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "\tw2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "\tunion = w1*h1 + w2*h2 - intersect\n",
        "\treturn(float(intersect) / union)\n",
        "\n",
        "def draw_boxes(image, boxes, labels):\n",
        "\timage_h, image_w, _ = image.shape\n",
        "\tfor box in boxes:\n",
        "\t\txmin = int(box.xmin*image_w)\n",
        "\t\tymin = int(box.ymin*image_h)\n",
        "\t\txmax = int(box.xmax*image_w)\n",
        "\t\tymax = int(box.ymax*image_h)\n",
        "\t\tcv2.rectangle(image, (xmin,ymin), (xmax,ymax), (0,255,0), 3)\n",
        "\t\tcv2.putText(image,\n",
        "\t\t\t\t\tlabels[box.get_label()] + ' ' + str(box.get_score()),\n",
        "\t\t\t\t\t(xmin, ymin - 13),\n",
        "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t\t\t\t\t1e-3 * image_h,\n",
        "\t\t\t\t\t(0,255,0), 2)\n",
        "\treturn(image)\n",
        "\n",
        "def _softmax(x, axis=-1, t=-100.):\n",
        "\tx = x - np.max(x)\n",
        "\tif np.min(x) < t:\n",
        "\t\tx = x/np.min(x)*t\n",
        "\te_x = np.exp(x)\n",
        "\treturn(e_x / e_x.sum(axis, keepdims=True))\n",
        "\n",
        "def decode_netout(netout, anchors, nb_class, obj_threshold=0.3, nms_threshold=0.3):\n",
        "\tgrid_h, grid_w, nb_box = netout.shape[:3]\n",
        "\tboxes = []\n",
        "\tnetout[..., 4]  = _sigmoid(netout[..., 4])\n",
        "\tnetout[..., 5:] = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n",
        "\tnetout[..., 5:] *= netout[..., 5:] > obj_threshold\n",
        "\tfor row in range(grid_h):\n",
        "\t\tfor col in range(grid_w):\n",
        "\t\t\tfor b in range(nb_box):\n",
        "\t\t\t\tclasses = netout[row,col,b,5:]\n",
        "\t\t\t\tif np.sum(classes) > 0:\n",
        "\t\t\t\t\tx, y, w, h = netout[row,col,b,:4]\n",
        "\t\t\t\t\tx = (col + _sigmoid(x)) / grid_w\n",
        "\t\t\t\t\ty = (row + _sigmoid(y)) / grid_h\n",
        "\t\t\t\t\tw = anchors[2 * b + 0] * np.exp(w) / grid_w\n",
        "\t\t\t\t\th = anchors[2 * b + 1] * np.exp(h) / grid_h\n",
        "\t\t\t\t\tconfidence = netout[row, col, b, 4]\n",
        "\t\t\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, confidence, classes)\n",
        "\t\t\t\t\tboxes.append(box)\n",
        "\tfor c in range(nb_class):\n",
        "\t\tsorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n",
        "\t\tfor i in range(len(sorted_indices)):\n",
        "\t\t\tindex_i = sorted_indices[i]\n",
        "\t\t\tif boxes[index_i].classes[c] == 0:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor j in range(i+1, len(sorted_indices)):\n",
        "\t\t\t\t\tindex_j = sorted_indices[j]\n",
        "\t\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j])>=nms_threshold:\n",
        "\t\t\t\t\t\tboxes[index_j].classes[c] = 0\n",
        "\tboxes = [box for box in boxes if box.get_score() > obj_threshold]\n",
        "\treturn(boxes)\n",
        "\n",
        "def compute_overlap(a, b):\n",
        "\tarea = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
        "\tiw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
        "\tih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
        "\tiw = np.maximum(iw, 0)\n",
        "\tih = np.maximum(ih, 0)\n",
        "\tua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
        "\tua = np.maximum(ua, np.finfo(float).eps)\n",
        "\tintersection = iw * ih\n",
        "\treturn(intersection / ua)\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "\tmrec = np.concatenate(([0.], recall, [1.]))\n",
        "\tmpre = np.concatenate(([0.], precision, [0.]))\n",
        "\tfor i in range(mpre.size - 1, 0, -1):\n",
        "\t\tmpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\ti = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\tap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "\treturn(ap)\n",
        "\n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "\tx1, x2 = interval_a\n",
        "\tx3, x4 = interval_b\n",
        "\tif x3 < x1:\n",
        "\t\tif x4 < x1:\n",
        "\t\t\treturn(0)\n",
        "\t\telse:\n",
        "\t\t\treturn(min(x2,x4) - x1)\n",
        "\telse:\n",
        "\t\tif x2 < x3:\n",
        "\t\t\t\treturn(0)\n",
        "\t\telse:\n",
        "\t\t\treturn(min(x2,x4) - x3)\n",
        "\n",
        "def _sigmoid(x):\n",
        "\treturn(1. / (1. + np.exp(-x)))\n",
        "\n",
        "def normal(image): return image/255.\n",
        "\n",
        "def space_to_depth_x2(x): return tf.space_to_depth(x, block_size=2)\n",
        "\n",
        "def parse_annotation(ann_dir, img_dir, labels=[]):\n",
        "\tall_imgs = []\n",
        "\tseen_labels = {}\n",
        "\tfor ann in sorted(os.listdir(ann_dir)):\n",
        "\t\timg = {'object':[]}\n",
        "\t\ttree = ET.parse(ann_dir + ann)\n",
        "\t\tfor elem in tree.iter():\n",
        "\t\t\tif 'filename' in elem.tag:\n",
        "\t\t\t\timg['filename'] = img_dir + elem.text\n",
        "\t\t\tif 'width' in elem.tag:\n",
        "\t\t\t\timg['width'] = int(elem.text)\n",
        "\t\t\tif 'height' in elem.tag:\n",
        "\t\t\t\timg['height'] = int(elem.text)\n",
        "\t\t\tif 'object' in elem.tag or 'part' in elem.tag:\n",
        "\t\t\t\tobj = {}\n",
        "\t\t\t\tfor attr in list(elem):\n",
        "\t\t\t\t\tif 'name' in attr.tag:\n",
        "\t\t\t\t\t\tobj['name'] = attr.text\n",
        "\t\t\t\t\t\tif obj['name'] in seen_labels:\n",
        "\t\t\t\t\t\t\tseen_labels[obj['name']] += 1\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tseen_labels[obj['name']] = 1\n",
        "\t\t\t\t\t\tif len(labels) > 0 and obj['name'] not in labels:\n",
        "\t\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\timg['object'] += [obj]\n",
        "\t\t\t\t\tif 'bndbox' in attr.tag:\n",
        "\t\t\t\t\t\tfor dim in list(attr):\n",
        "\t\t\t\t\t\t\tif 'xmin' in dim.tag:\n",
        "\t\t\t\t\t\t\t\tobj['xmin'] = int(round(float(dim.text)))\n",
        "\t\t\t\t\t\t\tif 'ymin' in dim.tag:\n",
        "\t\t\t\t\t\t\t\tobj['ymin'] = int(round(float(dim.text)))\n",
        "\t\t\t\t\t\t\tif 'xmax' in dim.tag:\n",
        "\t\t\t\t\t\t\t\tobj['xmax'] = int(round(float(dim.text)))\n",
        "\t\t\t\t\t\t\tif 'ymax' in dim.tag:\n",
        "\t\t\t\t\t\t\t\tobj['ymax'] = int(round(float(dim.text)))\n",
        "\t\tif len(img['object']) > 0:\n",
        "\t\t\tall_imgs += [img]\n",
        "\treturn(all_imgs, seen_labels)\n",
        "\n",
        "class BatchGenerator(keras.utils.Sequence):\n",
        "\tdef __init__(self, images, config, shuffle=True, jitter=True, norm=None):\n",
        "\t\tself.generator = None\n",
        "\t\tself.images = images\n",
        "\t\tself.config = config\n",
        "\t\tself.shuffle = shuffle\n",
        "\t\tself.jitter = jitter\n",
        "\t\tself.norm = norm\n",
        "\t\tself.anchors = [BoundBox(0, 0, config['ANCHORS'][2*i], config['ANCHORS'][2*i+1]) for i in range(int(len(config['ANCHORS'])//2))]\n",
        "\t\tsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "\t\tself.aug_pipe = iaa.Sequential(\n",
        "\t\t\t[\n",
        "\t\t\t\tsometimes(iaa.Affine(\n",
        "\t\t\t\t)),\n",
        "\t\t\t\tiaa.SomeOf((0, 5),\n",
        "\t\t\t\t\t[\n",
        "\t\t\t\t\t\tiaa.OneOf([\n",
        "\t\t\t\t\t\t\tiaa.GaussianBlur((0, 3.0)),\n",
        "\t\t\t\t\t\t\tiaa.AverageBlur(k=(2, 7)),\n",
        "\t\t\t\t\t\t\tiaa.MedianBlur(k=(3, 11)),\n",
        "\t\t\t\t\t\t]),\n",
        "\t\t\t\t\t\tiaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n",
        "\t\t\t\t\t\tiaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
        "\t\t\t\t\t\tiaa.OneOf([\n",
        "\t\t\t\t\t\t\tiaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "\t\t\t\t\t\t]),\n",
        "\t\t\t\t\t\tiaa.Add((-10, 10), per_channel=0.5),\n",
        "\t\t\t\t\t\tiaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
        "\t\t\t\t\t\tiaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n",
        "\t\t\t\t\t],\n",
        "\t\t\t\t\trandom_order=True\n",
        "\t\t\t\t)\n",
        "\t\t\t],\n",
        "\t\t\trandom_order=True\n",
        "\t\t)\n",
        "\t\tif shuffle: np.random.shuffle(self.images)\n",
        "\tdef __len__(self):\n",
        "\t\treturn int(np.ceil(float(len(self.images))/self.config['BATCH_SIZE']))\n",
        "\tdef num_classes(self):\n",
        "\t\treturn len(self.config['LABELS'])\n",
        "\tdef size(self):\n",
        "\t\treturn len(self.images)\n",
        "\tdef load_annotation(self, i):\n",
        "\t\tannots = []\n",
        "\t\tfor obj in self.images[i]['object']:\n",
        "\t\t\tannot = [obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax'], self.config['LABELS'].index(obj['name'])]\n",
        "\t\t\tannots += [annot]\n",
        "\t\tif len(annots) == 0: annots = [[]]\n",
        "\t\treturn np.array(annots)\n",
        "\tdef load_image(self, i):\n",
        "\t\treturn cv2.imread(self.images[i]['filename'])\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tl_bound = idx*self.config['BATCH_SIZE']\n",
        "\t\tr_bound = (idx+1)*self.config['BATCH_SIZE']\n",
        "\t\tif r_bound > len(self.images):\n",
        "\t\t\tr_bound = len(self.images)\n",
        "\t\t\tl_bound = r_bound - self.config['BATCH_SIZE']\n",
        "\t\tinstance_count = 0\n",
        "\t\tx_batch = np.zeros((r_bound - l_bound, self.config['IMAGE_H'], self.config['IMAGE_W'], 3))\n",
        "\t\tb_batch = np.zeros((r_bound - l_bound, 1, 1, 1, self.config['TRUE_BOX_BUFFER'], 4))\n",
        "\t\ty_batch = np.zeros((r_bound - l_bound, self.config['GRID_H'], self.config['GRID_W'], self.config['BOX'], 4+1+len(self.config['LABELS'])))\n",
        "\t\tfor train_instance in self.images[l_bound:r_bound]:\n",
        "\t\t\timg, all_objs = self.aug_image(train_instance, jitter=self.jitter)\n",
        "\t\t\ttrue_box_index = 0\n",
        "\t\t\tfor obj in all_objs:\n",
        "\t\t\t\tif obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin'] and obj['name'] in self.config['LABELS']:\n",
        "\t\t\t\t\tcenter_x = .5*(obj['xmin'] + obj['xmax'])\n",
        "\t\t\t\t\tcenter_x = center_x / (float(self.config['IMAGE_W']) / self.config['GRID_W'])\n",
        "\t\t\t\t\tcenter_y = .5*(obj['ymin'] + obj['ymax'])\n",
        "\t\t\t\t\tcenter_y = center_y / (float(self.config['IMAGE_H']) / self.config['GRID_H'])\n",
        "\t\t\t\t\tgrid_x = int(np.floor(center_x))\n",
        "\t\t\t\t\tgrid_y = int(np.floor(center_y))\n",
        "\t\t\t\t\tif grid_x < self.config['GRID_W'] and grid_y < self.config['GRID_H']:\n",
        "\t\t\t\t\t\tobj_indx  = self.config['LABELS'].index(obj['name'])\n",
        "\t\t\t\t\t\tcenter_w = (obj['xmax'] - obj['xmin']) / (float(self.config['IMAGE_W']) / self.config['GRID_W'])\n",
        "\t\t\t\t\t\tcenter_h = (obj['ymax'] - obj['ymin']) / (float(self.config['IMAGE_H']) / self.config['GRID_H'])\n",
        "\t\t\t\t\t\tbox = [center_x, center_y, center_w, center_h]\n",
        "\t\t\t\t\t\tbest_anchor = -1\n",
        "\t\t\t\t\t\tmax_iou     = -1\n",
        "\t\t\t\t\t\tshifted_box = BoundBox(\t0,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t0,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tcenter_w,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tcenter_h)\n",
        "\t\t\t\t\t\tfor i in range(len(self.anchors)):\n",
        "\t\t\t\t\t\t\tanchor= self.anchors[i]\n",
        "\t\t\t\t\t\t\tiou = bbox_iou(shifted_box, anchor)\n",
        "\t\t\t\t\t\t\tif max_iou < iou:\n",
        "\t\t\t\t\t\t\t\tbest_anchor = i\n",
        "\t\t\t\t\t\t\t\tmax_iou     = iou\n",
        "\t\t\t\t\t\ty_batch[instance_count, grid_y, grid_x, best_anchor, 0:4] = box\n",
        "\t\t\t\t\t\ty_batch[instance_count, grid_y, grid_x, best_anchor, 4] = 1.\n",
        "\t\t\t\t\t\ty_batch[instance_count, grid_y, grid_x, best_anchor, 5+obj_indx] = 1\n",
        "\t\t\t\t\t\tb_batch[instance_count, 0, 0, 0, true_box_index] = box\n",
        "\t\t\t\t\t\ttrue_box_index += 1\n",
        "\t\t\t\t\t\ttrue_box_index = true_box_index % self.config['TRUE_BOX_BUFFER']\n",
        "\t\t\tif self.norm != None:\n",
        "\t\t\t\tx_batch[instance_count] = self.norm(img)\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor obj in all_objs:\n",
        "\t\t\t\t\tif obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin']:\n",
        "\t\t\t\t\t\tcv2.rectangle(img[:,:,::-1], (obj['xmin'],obj['ymin']), (obj['xmax'],obj['ymax']), (255,0,0), 3)\n",
        "\t\t\t\t\t\tcv2.putText(img[:,:,::-1], obj['name'],\n",
        "\t\t\t\t\t\t\t\t\t(obj['xmin']+2, obj['ymin']+12),\n",
        "\t\t\t\t\t\t\t\t\t0, 1.2e-3 * img.shape[0],\n",
        "\t\t\t\t\t\t\t\t\t(0,255,0), 2)\n",
        "\t\t\t\tx_batch[instance_count] = img\n",
        "\t\t\tinstance_count += 1\n",
        "\t\treturn [x_batch, b_batch], y_batch\n",
        "\tdef on_epoch_end(self):\n",
        "\t\tif self.shuffle: np.random.shuffle(self.images)\n",
        "\tdef aug_image(self, train_instance, jitter):\n",
        "\t\timage_name = train_instance['filename']\n",
        "\t\timage = cv2.imread(image_name)\n",
        "\t\tif image is None: print('Cannot find ', image_name)\n",
        "\t\th, w, c = image.shape\n",
        "\t\tall_objs = copy.deepcopy(train_instance['object'])\n",
        "\t\tif jitter:\n",
        "\t\t\tscale = np.random.uniform() / 10. + 1.\n",
        "\t\t\timage = cv2.resize(image, (0,0), fx = scale, fy = scale)\n",
        "\t\t\tmax_offx = (scale-1.) * w\n",
        "\t\t\tmax_offy = (scale-1.) * h\n",
        "\t\t\toffx = int(np.random.uniform() * max_offx)\n",
        "\t\t\toffy = int(np.random.uniform() * max_offy)\n",
        "\t\t\timage = image[offy : (offy + h), offx : (offx + w)]\n",
        "\t\t\tflip = np.random.binomial(1, .5)\n",
        "\t\t\tif flip > 0.5: image = cv2.flip(image, 1)\n",
        "\t\t\timage = self.aug_pipe.augment_image(image)\n",
        "\t\timage = cv2.resize(image, (self.config['IMAGE_H'], self.config['IMAGE_W']))\n",
        "\t\timage = image[:,:,::-1]\n",
        "\t\tfor obj in all_objs:\n",
        "\t\t\tfor attr in ['xmin', 'xmax']:\n",
        "\t\t\t\tif jitter: obj[attr] = int(obj[attr] * scale - offx)\n",
        "\t\t\t\tobj[attr] = int(obj[attr] * float(self.config['IMAGE_W']) / w)\n",
        "\t\t\t\tobj[attr] = max(min(obj[attr], self.config['IMAGE_W']), 0)\n",
        "\t\t\tfor attr in ['ymin', 'ymax']:\n",
        "\t\t\t\tif jitter: obj[attr] = int(obj[attr] * scale - offy)\n",
        "\t\t\t\tobj[attr] = int(obj[attr] * float(self.config['IMAGE_H']) / h)\n",
        "\t\t\t\tobj[attr] = max(min(obj[attr], self.config['IMAGE_H']), 0)\n",
        "\t\t\tif jitter and flip > 0.5:\n",
        "\t\t\t\txmin = obj['xmin']\n",
        "\t\t\t\tobj['xmin'] = self.config['IMAGE_W'] - obj['xmax']\n",
        "\t\t\t\tobj['xmax'] = self.config['IMAGE_W'] - xmin\n",
        "\t\treturn(image, all_objs)\n",
        "\n",
        "all_imgs, seen_labels = parse_annotation(annot_path, image_path)\n",
        "for img in all_imgs: img['filename'] = img['filename']# + '.jpg'\n",
        "batches = BatchGenerator(all_imgs, GenConf)\n",
        "image = batches[0][0][0][0]\n",
        "train_valid_split = int(0.8*len(all_imgs))\n",
        "train_batch = BatchGenerator(all_imgs[:train_valid_split], GenConf, norm=normal)\n",
        "valid_batch = BatchGenerator(all_imgs[train_valid_split:], GenConf, norm=normal)\n",
        "input_image = keras.layers.Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "true_boxes  = keras.layers.Input(shape=(1, 1, 1, TRUE_BOX_BUFFER, 4))\n",
        "\n",
        "x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
        "x = BatchNormalization(name='norm_1')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_2')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_3')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_4')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_5')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_6')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_7')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False, input_shape=(416,416,3))(x)\n",
        "x = BatchNormalization(name='norm_8')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_9')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_10')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_11')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_12')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_13')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "skip_connection = x\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_14')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_15')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_16')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_17')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_18')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_19')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_20')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
        "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
        "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "x = concatenate([skip_connection, x])\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_22')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
        "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
        "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
        "model = keras.models.Model([input_image, true_boxes], output)\n",
        "#model.summary()\n",
        "\n",
        "weight_reader = WeightReader(wt_path)\n",
        "weight_reader.reset()\n",
        "nb_conv = 23\n",
        "for i in range(1, nb_conv+1):\n",
        "\tconv_layer = model.get_layer('conv_' + str(i))\n",
        "\tif i < nb_conv:\n",
        "\t\tnorm_layer = model.get_layer('norm_' + str(i))\n",
        "\t\tsize = np.prod(norm_layer.get_weights()[0].shape)\n",
        "\t\tbeta = weight_reader.read_bytes(size)\n",
        "\t\tgamma = weight_reader.read_bytes(size)\n",
        "\t\tmean = weight_reader.read_bytes(size)\n",
        "\t\tvar = weight_reader.read_bytes(size)\n",
        "\t\tweights = norm_layer.set_weights([gamma, beta, mean, var])\n",
        "\tif len(conv_layer.get_weights()) > 1:\n",
        "\t\tbias = weight_reader.read_bytes(np.prod(conv_layer.get_weights() [1].shape))\n",
        "\t\tkernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights() [0].shape))\n",
        "\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights() [0].shape)))\n",
        "\t\tkernel = kernel.transpose([2,3,1,0])\n",
        "\t\tconv_layer.set_weights([kernel, bias])\n",
        "\telse:\n",
        "\t\tkernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights() [0].shape))\n",
        "\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights() [0].shape)))\n",
        "\t\tkernel = kernel.transpose([2,3,1,0])\n",
        "\t\tconv_layer.set_weights([kernel])\n",
        "layer\t\t= model.layers[-4]\n",
        "weights\t\t= layer.get_weights()\n",
        "new_kernel\t= np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\n",
        "new_bias\t= np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n",
        "layer.set_weights([new_kernel, new_bias])\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "\tmask_shape\t\t= tf.shape(y_true)[:4]\n",
        "\tcell_x\t\t\t= tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
        "\tcell_y\t\t\t= tf.transpose(cell_x, (0,2,1,3,4))\n",
        "\tcell_grid\t\t= tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
        "\tcoord_mask\t\t= tf.zeros(mask_shape)\n",
        "\tconf_mask\t\t= tf.zeros(mask_shape)\n",
        "\tclass_mask\t\t= tf.zeros(mask_shape)\n",
        "\tseen\t\t\t= tf.Variable(0.)\n",
        "\ttotal_recall\t= tf.Variable(0.)\n",
        "\tpred_box_xy\t\t= tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
        "\tpred_box_wh\t\t= tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
        "\tpred_box_conf\t= tf.sigmoid(y_pred[..., 4])\n",
        "\tpred_box_class\t= y_pred[..., 5:]\n",
        "\ttrue_box_xy\t\t= y_true[..., 0:2]\n",
        "\ttrue_box_wh\t\t= y_true[..., 2:4]\n",
        "\ttrue_wh_half\t= true_box_wh / 2.\n",
        "\ttrue_mins\t\t= true_box_xy - true_wh_half\n",
        "\ttrue_maxes\t\t= true_box_xy + true_wh_half\n",
        "\tpred_wh_half\t= pred_box_wh / 2.\n",
        "\tpred_mins\t\t= pred_box_xy - pred_wh_half\n",
        "\tpred_maxes\t\t= pred_box_xy + pred_wh_half\n",
        "\tintersect_mins\t= tf.maximum(pred_mins,  true_mins)\n",
        "\tintersect_maxes\t= tf.minimum(pred_maxes, true_maxes)\n",
        "\tintersect_wh\t= tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "\tintersect_areas\t= intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\ttrue_areas\t\t= true_box_wh[..., 0] * true_box_wh[..., 1]\n",
        "\tpred_areas\t\t= pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
        "\tunion_areas\t\t= pred_areas + true_areas - intersect_areas\n",
        "\tiou_scores\t\t= tf.truediv(intersect_areas, union_areas)\n",
        "\ttrue_box_conf\t= iou_scores * y_true[..., 4]\n",
        "\ttrue_box_class\t= tf.argmax(y_true[..., 5:], -1)\n",
        "\tcoord_mask\t\t= tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
        "\ttrue_xy\t\t\t= true_boxes[..., 0:2]\n",
        "\ttrue_wh\t\t\t= true_boxes[..., 2:4]\n",
        "\ttrue_wh_half\t= true_wh / 2.\n",
        "\ttrue_mins\t\t= true_xy - true_wh_half\n",
        "\ttrue_maxes\t\t= true_xy + true_wh_half\n",
        "\tpred_xy\t\t\t= tf.expand_dims(pred_box_xy, 4)\n",
        "\tpred_wh\t\t\t= tf.expand_dims(pred_box_wh, 4)\n",
        "\tpred_wh_half\t= pred_wh / 2.\n",
        "\tpred_mins\t\t= pred_xy - pred_wh_half\n",
        "\tpred_maxes\t\t= pred_xy + pred_wh_half\n",
        "\tintersect_mins\t= tf.maximum(pred_mins,  true_mins)\n",
        "\tintersect_maxes\t= tf.minimum(pred_maxes, true_maxes)\n",
        "\tintersect_wh\t= tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "\tintersect_areas\t= intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\ttrue_areas\t\t= true_wh[..., 0] * true_wh[..., 1]\n",
        "\tpred_areas\t\t= pred_wh[..., 0] * pred_wh[..., 1]\n",
        "\tunion_areas\t\t= pred_areas + true_areas - intersect_areas\n",
        "\tiou_scores\t\t= tf.truediv(intersect_areas, union_areas)\n",
        "\tbest_ious\t\t= tf.reduce_max(iou_scores, axis=4)\n",
        "\tconf_mask\t\t= conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
        "\tconf_mask\t\t= conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
        "\tclass_mask\t\t= y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE\n",
        "\tno_boxes_mask\t= tf.to_float(coord_mask < COORD_SCALE/2.)\n",
        "\tseen\t\t\t= tf.assign_add(seen, 1.)\n",
        "\ttrue_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, tf.ones_like(coord_mask)], lambda: [true_box_xy, true_box_wh, coord_mask])\n",
        "\tnb_coord_box\t= tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
        "\tnb_conf_box\t\t= tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
        "\tnb_class_box\t= tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
        "\tloss_xy\t\t\t= tf.reduce_sum(tf.square(true_box_xy-pred_box_xy) * coord_mask)\t/\t(nb_coord_box + 1e-6) / 2.\n",
        "\tloss_wh\t\t\t= tf.reduce_sum(tf.square(true_box_wh-pred_box_wh) * coord_mask)\t/\t(nb_coord_box + 1e-6) / 2.\n",
        "\tloss_conf\t\t= tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)\t/\t(nb_conf_box  + 1e-6) / 2.\n",
        "\tloss_class\t\t= tf.nn.sparse_softmax_cross_entropy_with_logits (labels=true_box_class, logits=pred_box_class)\n",
        "\tloss_class\t\t= tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
        "\tloss = loss_xy + loss_wh + loss_conf + loss_class\n",
        "\tnb_true_box = tf.reduce_sum(y_true[..., 4])\n",
        "\tnb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
        "\tcurrent_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
        "\ttotal_recall = tf.assign_add(total_recall, current_recall)\n",
        "\tloss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
        "\tloss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
        "\tloss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
        "\tloss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
        "\tloss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
        "\tloss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
        "\tloss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
        "\tloss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
        "\treturn(loss)\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, mode='min', verbose=1)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
        "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs/', histogram_freq=0, write_graph=True, write_images=False)\n",
        "#tb_counter = len([log for log in os.listdir(os.path.expanduser('./logs/')) if 'cell' in log]) + 1\n",
        "optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
        "#optimizer = RMSprop(lr=1e-5, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss=custom_loss, optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5MF__wQdrU0I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train"
      ]
    },
    {
      "metadata": {
        "id": "6JtDZO6xIZBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator\t\t= train_batch,\n",
        "                    steps_per_epoch\t= len(train_batch),\n",
        "                    epochs\t\t\t= 100,\n",
        "                    verbose\t\t\t= 1,\n",
        "                    validation_data\t= valid_batch,\n",
        "                    validation_steps= len(valid_batch),\n",
        "                    callbacks\t\t= [early_stop, checkpoint, tensorboard],\n",
        "                    max_queue_size\t= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSQTXkrArXDb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Detect"
      ]
    },
    {
      "metadata": {
        "id": "TRUBniHCTzyC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('weights.h5')\n",
        "dummy_array = np.zeros((1, 1, 1, 1, TRUE_BOX_BUFFER, 4))\n",
        "image = cv2.imread('./dataset/images/Cell001.jpg')\n",
        "plt.figure(figsize=(10,10))\n",
        "input_image = cv2.resize(image, (416, 416))\n",
        "input_image = input_image / 255.\n",
        "input_image = input_image[:,:,::-1]\n",
        "input_image = np.expand_dims(input_image, 0)\n",
        "netout = model.predict([input_image, dummy_array])\n",
        "boxes = decode_netout(netout[0], obj_threshold=0.5, nms_threshold=NMS_THRESHOLD, anchors=ANCHORS, nb_class=CLASS)\n",
        "print(len(boxes))\n",
        "image = draw_boxes(image, boxes, labels=LABELS)\n",
        "plt.imshow(image[:,:,::-1]); plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggpSFUN9lPO1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm weights.h5"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}